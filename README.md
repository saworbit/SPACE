# SPACE MVP - Storage Platform for Adaptive Computational Ecosystems

> **One capsule. Infinite views.** The future of storage starts with a single primitive that breaks down protocol silos.

[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![Rust](https://img.shields.io/badge/rust-1.78%2B-orange.svg)](https://www.rust-lang.org)
[![Status](https://img.shields.io/badge/status-Phase%203.3%20Advanced%20Security-green.svg)](https://github.com/your-org/space)

---

## The Big Idea

Traditional storage forces you into boxes: **block** *or* **file** *or* **object**. Different APIs, separate data copies, endless complexity.

**SPACE flips the script.** Everything is a **capsule** -- a universal 128-bit ID that can be viewed through *any* protocol:

| Protocol | Access Method |
|----------|---------------|
| **Block** | NVMe-oF, iSCSI |
| **File** | NFS, SMB |
| **Object** | S3 API |

**The same capsule. Three different views. Zero data copies.**

---

## Current Status: Phase 3.3 - Advanced Security Landed

**Status:** Phase 3.3 Complete - Advanced Security hardened!

**What exists NOW:**
- Universal capsule storage with persistent metadata
- CLI create/read operations
- S3-compatible REST API (protocol view proof-of-concept)
- NFS + block protocol views (namespace + volume facades)
- Adaptive compression (LZ4/Zstd with entropy detection)
- Zero-copy compression/dedup pipeline using `Cow<[u8]>` + `bytes::Bytes` shared buffers
- Content-addressed deduplication (post-compression)
- **XTS-AES-256 encryption with BLAKE3-MAC integrity**
- **Deterministic encryption preserving deduplication**
- **Key management with rotation support**
- **Reference-counted garbage collection with metadata reclamation**
- **Modular trait-based pipeline for read/delete/GC (feature `modular_pipeline`)**
- **Tokio-powered async write pipeline** (Cargo feature `pipeline_async`) with staged NVRAM transactions, bounded concurrency, and `tracing` metrics
- **Counting Bloom filters** in the registry to prescreen dedup candidates at multi-million scale
- **Immutable audit log** with BLAKE3 hash chaining + optional TSA anchoring (`security::audit_log`)
- **SPIFFE + mTLS eBPF gateway** when the `advanced-security` feature is enabled (`protocol-s3`)
- **Post-quantum crypto toggle** (Kyber + AES hybrid) selectable via `Policy::crypto_profile`
- **Dedicated `security` module** so Bloom/audit/PQ/eBPF logic stays feature gated

**What's coming next:**
- ? Replication & clustering
- ? Policy compiler

## What This MVP Proves

**Phase 3.3 Complete: Compression ? | Dedup ? | Protocol Views ? | Advanced Security ?**

### Phase 1: Core Storage [x]
[x] **Universal Capsule IDs** -- 128-bit UUIDs as the single storage primitive  
[x] **Persistent NVRAM Log** -- Append-only durability with automatic fsync  
[x] **Intelligent Segmentation** -- Auto-split to 4MB chunks for efficiency  
[x] **CLI Tool** -- Create and read capsules from the command line  
[x] **JSON Metadata** -- Human-readable registry for debugging and inspection  

### Phase 2.1: Compression [x]
[x] **LZ4 Fast Compression** -- Sub-millisecond compression for hot data  
[x] **Zstd Balanced Compression** -- High compression ratios for cold data  
[x] **Entropy Detection** -- Skip compression on random/pre-compressed data  
[x] **Policy-Driven** -- Configure compression per capsule with presets  
[x] **Zero-Copy Fast-Path** -- Borrow slices when compression can reuse input to avoid extra allocations  

### Phase 2.2: Deduplication [x]
[x] **Content-Addressed Storage** -- BLAKE3 hashing of compressed segments  
[x] **Automatic Dedup** -- Reuse identical segments across capsules  
[x] **Space Savings Tracking** -- Monitor dedup ratios and bytes saved  
[x] **Post-Compression Dedup** -- Foundation for "dedupe over ciphertext"  
[x] **Borrowed Hashing Path** -- Zero-copy buffers flow through hashing/encryption without cloning  

### Phase 2.3: Protocol Views [x]
[x] **S3 REST API** -- PUT/GET/HEAD/LIST/DELETE operations  
[x] **Protocol Abstraction** -- Same capsule accessible via multiple APIs  
[x] **NFS namespace view** - Hierarchical directories backed by capsules  
[x] **Block volume view** - Logical LUN facade with copy-on-write rewrites  

### Phase 3.1: Encryption & Integrity [x]
[x] **XTS-AES-256 Encryption** -- Per-segment encryption with hardware acceleration  
[x] **BLAKE3-MAC Integrity** -- Tamper detection with keyed MAC  
[x] **Deterministic Encryption** -- Content-derived tweaks preserve deduplication  
[x] **Key Management** -- Version-tracked key derivation with rotation support  
[x] **Zero-Trust Design** -- Keys from environment, zeroized on drop  

### Phase 3.3: Advanced Security
- **Counting Bloom filters** guard the registry from multi-million entry dedup explosions while keeping false positives ~0.1%.
- **Immutable audit log** persists every capsule/segment event with BLAKE3 hash chaining plus optional TSA webhooks (`security::audit_log`).
- **Zero-trust ingress**  the SPIFFE + mTLS gateway (feature `advanced-security`) layers an eBPF policy filter and refreshable workload allow-list.
- **Post-quantum crypto toggle**  `Policy::crypto_profile = HybridKyber` wraps AES keys with Kyber ML-KEM material for forward secrecy.
- **Modular security crate** keeps Bloom/Audit/PQ/eBPF code feature gated so sovereign deployments can opt in/out cleanly.

---

## Quick Start

### System Requirements
- Linux, macOS, or Windows
- Rust 1.78+
- 2GB free disk space

### Build
```bash
cargo build --release
```

### Setup Encryption (Optional)
```bash
# Generate master key for encryption
export SPACE_MASTER_KEY=$(openssl rand -hex 32)

# Verify setup
echo ${#SPACE_MASTER_KEY}  # Should output 64
```

### Advanced Security Setup (Optional)
```bash
# Opt-in to Bloom/audit/SPIFFE/PQ via the feature flag
cargo build --features advanced-security

# Registry tuning (optional)
export SPACE_BLOOM_CAPACITY=10000000        # default: 10M entries
export SPACE_BLOOM_FPR=0.001                # default: 0.1% false positives

# Audit log (optional TSA batches every 100 events)
export SPACE_AUDIT_LOG=/var/lib/space/space.audit.log
export SPACE_AUDIT_FLUSH=5                  # fsync every 5 events
export SPACE_TSA_ENDPOINT=https://tsa.local/submit
export SPACE_TSA_API_KEY=demo-token

# SPIFFE + mTLS ingress (protocol-s3)
export SPACE_ALLOWED_SPIFFE_IDS="spiffe://demo/client-a,spiffe://demo/client-b"
export SPACE_SPIFFE_ENDPOINT=ws://127.0.0.1:9001/identities
export SPACE_SPIFFE_HEADER=x-spiffe-id
export SPACE_SPIFFE_REFRESH_SECS=30
export SPACE_BPF_PROGRAM=/opt/space/gateway.bpf.o   # optional on Linux

# Kyber hybrid toggle for PQ readiness
export SPACE_KYBER_KEY_PATH=/var/lib/space/space.kyber.key
```

Run the zero-trust S3 test on Linux (aya/ebpf requires a unix target):
```bash
cargo test -p protocol-s3 --features advanced-security
```

### Create a Capsule
```bash
# Basic usage (no encryption)
echo "Hello SPACE!" > test.txt
./target/release/spacectl create --file test.txt

# Output:
# [x] Capsule created: 550e8400-e29b-41d4-a716-446655440000
#    Size: 13 bytes
#     Segment 0: 1.85x compression (13 -> 7 bytes, lz4_1)
# [x] Capsule 550e8400-...: 1.85x compression, 0 dedup hits
```

### Read It Back
```bash
# Replace UUID with your capsule ID
./target/release/spacectl read 550e8400-e29b-41d4-a716-446655440000 > output.txt
```

### Test Deduplication
```bash
# Create file with repeated content (Bash)
echo "SPACE STORAGE " > test_repeated.txt
for i in {1..5000}; do echo "SPACE STORAGE " >> test_repeated.txt; done

# PowerShell alternative:
# "SPACE STORAGE " * 5000 | Out-File test_repeated.txt

# Create first capsule
./target/release/spacectl create --file test_repeated.txt

# Create second capsule (same content - watch for dedup!)
./target/release/spacectl create --file test_repeated.txt

# Expected Output:
# *  Dedup hit: Reusing segment 1 (saved 4194304 bytes)
# [x] Capsule ...: 5.23x compression, 1 dedup hits (4194304 bytes saved)
```

### Enable Async Pipeline & Metrics (optional)
```bash
# Build with async pipeline enabled
cargo build --features pipeline_async

# Run CLI with runtime-managed async pipeline and info-level tracing
RUST_LOG=info ./target/debug/spacectl create --file test.txt

# Run feature-gated tests
cargo test -p capsule-registry --features pipeline_async
```

### Opt in to the Modular Pipeline (compression/dedup/encryption traits)
```bash
# Build everything with the modular orchestrator available
cargo build --features modular_pipeline

# Create or read capsules via the trait-based pipeline
./target/release/spacectl create --file demo.txt --modular
./target/release/spacectl read 550e8400-e29b-41d4-a716-446655440000 --modular > output.txt

# Serve the S3 view against the modular backend
./target/release/spacectl serve-s3 --port 8080 --modular

# Legacy callers can still flip back at runtime, even when the feature is enabled
SPACE_DISABLE_MODULAR_PIPELINE=1 ./target/release/spacectl create --file demo.txt
```

The modular path instantiates `compression`, `dedup`, `encryption`, and `storage` crates through shared traits, while `WritePipeline` automatically delegates reads/writes/GC to the new orchestrator whenever the feature is compiled in. Protocol crates (e.g., S3) and the CLI share a common helper (`registry_pipeline_from_env`) so they all exercise the same code paths. Disable the feature entirely for leaner binaries via `--no-default-features` or by omitting `--features modular_pipeline`.

### Start S3 Server
```bash
./target/release/spacectl serve-s3 --port 8080

# In another terminal, test S3 API
curl -X PUT http://localhost:8080/demo-bucket/hello.txt -d "Hello from S3!"
curl http://localhost:8080/demo-bucket/hello.txt
```

---

## Architecture
```
+-------------------------------------------------------------+
|                      spacectl (CLI)                          |
|           Your interface to the storage fabric               |
+--------------------+----------------------------------------+
                     |
+--------------------v----------------------------------------+
|                CapsuleRegistry                               |
|      Manages capsule metadata & segment mappings            |
|      Content Store: ContentHash -> SegmentId                 |
+--------------------------------------------------------------+
|                WritePipeline                                 |
|   Segments -> Compress -> Hash -> Encrypt -> MAC -> Dedup -> Store|
+--------------------+----------------------------------------+
                     |
+--------------------v----------------------------------------+
|                   NvramLog                                   |
|         Durable append-only segment storage                  |
+--------------------------------------------------------------+
```

### Data Flow (Write Path with Compression, Encryption & Dedup)
```
Input File
    |
    +-> Split into 4MB segments
    |
    +-> Compress each segment (LZ4/Zstd)
    |   +-> Skip if high entropy (random data)
    |
    +-> Hash compressed data (BLAKE3)
    |
    +-> Encrypt (if enabled)
    |   +- Derive deterministic tweak from hash
    |   +- XTS-AES-256 encryption
    |
    +-> Compute MAC (BLAKE3-keyed)
    |
    +-> Check content store
    |   +- Hit?  -> Reuse existing segment (dedup!)
    |   +- Miss? -> Write new segment
    |
    +-> Append to NVRAM log (fsync)
    |
    +-> Update metadata registry
         |
         +-> Return CapsuleID to user
```

---

## Project Structure
```
space/
+-- crates/
|   +-- common/              # Shared types (CapsuleId, SegmentId, Policy)
|   +-- encryption/          # NEW: XTS-AES-256 + BLAKE3-MAC + Key management
|   |   +-- src/
|   |   |   +-- lib.rs       # Module exports
|   |   |   +-- error.rs     # Error types
|   |   |   +-- policy.rs    # EncryptionPolicy & metadata
|   |   |   +-- keymanager.rs# Key derivation & rotation
|   |   |   +-- xts.rs       # XTS-AES-256 encryption
|   |   |   +-- mac.rs       # BLAKE3-MAC integrity
|   |   +-- tests/           # 53 passing tests
|   +-- capsule-registry/    # Metadata + write pipeline + dedup + encryption
|   |   +-- src/
|   |   |   +-- lib.rs       # Registry with content store
|   |   |   +-- pipeline.rs  # Write/read with encryption integration
|   |   |   +-- compression.rs # LZ4/Zstd adaptive compression
|   |   |   +-- dedup.rs     # BLAKE3 hashing & stats
|   |   +-- tests/
|   |       +-- integration_test.rs
|   |       +-- dedup_test.rs
|   +-- nvram-sim/           # Persistent log storage simulator
|   +-- protocol-s3/         # S3-compatible REST API
|   +-- spacectl/            # Command-line interface
+-- docs/
|   +-- architecture.md
|   +-- patentable_concepts.md
|   +-- future_state_architecture.md
|   +-- DEDUP_IMPLEMENTATION.md        # Phase 2.2 details
|   +-- ENCRYPTION_IMPLEMENTATION.md   # NEW: Phase 3 details
+-- Cargo.toml               # Workspace configuration
+-- demo_s3.sh               # S3 protocol demo
+-- test_dedup.sh            # Deduplication demo (Bash)
+-- README.md                # You are here
```

### Runtime Files (Auto-Generated)
```
space.metadata         -> Capsule registry + content store (JSON)
space.nvram            -> Raw segment data (encrypted if enabled)
space.nvram.segments   -> Segment metadata with encryption info (JSON)
```

---

## Testing
```bash
# Run all tests
cargo test --workspace

# Run with output to see compression/dedup/encryption stats
cargo test --workspace -- --nocapture

# Run encryption tests
cargo test -p encryption -- --nocapture

# Run dedup-specific tests
cargo test --test dedup_test -- --nocapture

# Exercise the modular pipeline prototype (compression/dedup/encryption traits)
cargo test -p capsule-registry --features modular_pipeline -- --nocapture

# Run S3 protocol tests
cargo test -p protocol-s3 -- --nocapture

# Zero-trust ingress tests (Linux + advanced-security feature)
cargo test -p protocol-s3 --features advanced-security -- --nocapture


# Automated dedup demo (Linux/macOS/Git Bash)
./test_dedup.sh

# Automated dedup demo (Windows PowerShell)
.\test_dedup.ps1
```

**Test Coverage:**
- [x] Write/read round-trip with compression
- [x] Multi-segment handling
- [x] Metadata persistence
- [x] NVRAM log recovery
- [x] Compression entropy detection
- [x] Deduplication across capsules
- [x] S3 protocol views (PUT/GET/HEAD/LIST/DELETE)
- [x] **Encryption/decryption round-trip**
- [x] **MAC integrity verification**
- [x] **Key derivation & rotation**
- [x] **Deterministic encryption for dedup**

---

## Why This Matters

### Traditional Storage Problems

| Problem | SPACE Solution |
|---------|----------------|
| Protocol lock-in (block vs file vs object) | **One capsule, multiple views** |
| Data duplication across tiers | **Content-addressed deduplication** |
| Complex migration between protocols | **Instant protocol switching** |
| Forklift upgrades required | **Microservice-based evolution** |
| Security bolted on afterward | **Built-in encryption per segment [x]** |
| Encryption breaks deduplication | **Deterministic tweaks preserve dedup [x]** |
| Wasted space on duplicate data | **Automatic dedup with 2-3x savings** |
| CPU overhead for compression | **Entropy detection skips random data** |
| No integrity verification | **BLAKE3-MAC on every segment [x]** |

### Proven Architecture

This MVP proves the core innovations outlined in the architecture documents:

 **Dedup Over Encrypted Data** -- Deterministic encryption preserves space efficiency  
 **Adaptive Compression** -- LZ4/Zstd with entropy-based selection  
 **Content-Addressed Storage** -- BLAKE3 hashing enables global dedup  
! **Protocol Views** -- S3 API proves universal namespace works  
 **Space Efficiency** -- 2-3x savings maintained with encryption  
 **Key Management** -- Version-tracked derivation with rotation  
[x] **Integrity Verification** -- BLAKE3-MAC detects tampering  

---

## Security & Encryption

### The Core Innovation

Traditional encryption **destroys** deduplication:
```
Plaintext A + Random IV -> Ciphertext X
Plaintext A + Random IV -> Ciphertext Y (different!)
Result: Dedup FAILS x
```

**SPACE's breakthrough:**
```
Plaintext A -> Compress -> Hash -> Deterministic Tweak -> Ciphertext X
Plaintext A -> Compress -> Hash -> Same Tweak         -> Ciphertext X [x]
Result: Dedup WORKS while maintaining encryption!
```

### Security Properties

| Property | Implementation | Strength |
|----------|----------------|----------|
| **Confidentiality** | XTS-AES-256 | 256-bit |
| **Integrity** | BLAKE3-MAC | 128-bit |
| **Deduplication** | Deterministic tweaks | Preserved |
| **Key Derivation** | BLAKE3-KDF | Cryptographic |
| **Key Rotation** | Version tracking | Zero downtime |
| **Memory Safety** | Zeroization | Keys cleared on drop |

### Quick Encryption Setup
```bash
# Generate 256-bit master key
export SPACE_MASTER_KEY=$(openssl rand -hex 32)

# Encryption now auto-enabled for all writes
# Read operations auto-decrypt when keys available
```

For detailed security documentation, see [ENCRYPTION_IMPLEMENTATION.md](docs/ENCRYPTION_IMPLEMENTATION.md)

---

## Roadmap

### [x] Phase 1: Core Storage (COMPLETE)
- [x] Capsule registry with persistent metadata
- [x] NVRAM log simulator
- [x] CLI for create/read operations
- [x] 4MB automatic segmentation
- [x] Integration tests

### [x] Phase 2.1: Compression (COMPLETE)
- [x] LZ4 fast compression
- [x] Zstd balanced compression
- [x] Entropy-based compression selection
- [x] Policy-driven compression levels
- [x] Compression statistics tracking

### [x] Phase 2.2: Deduplication (COMPLETE)
- [x] BLAKE3 content hashing
- [x] Content-addressed storage (ContentHash -> SegmentId)
- [x] Post-compression deduplication
- [x] Dedup statistics and monitoring
- [x] Reference counting (foundation for GC)

### [x] Phase 2.3: Protocol Views (COMPLETE)
- [x] S3-compatible REST API
- [x] PUT/GET/HEAD/LIST/DELETE operations
- [x] Protocol abstraction layer
- [x] S3 server with Axum

### [x] Phase 3.1: Encryption & Integrity (COMPLETE)
- [x] XTS-AES-256 per-segment encryption
- [x] Deterministic tweak derivation (preserves dedup)
- [x] BLAKE3-MAC integrity verification
- [x] Key management with BLAKE3-KDF
- [x] Key rotation with version tracking
- [x] Environment-based key configuration
- [x] Memory zeroization for security
- [x] 53 comprehensive tests

### Phase 3.2: Lifecycle Management (COMPLETE)
- [x] Reference-counted segment tracking across capsules
- [x] Startup refcount reconciliation on pipeline initialization
- [x] Manual garbage collector for metadata reclamation

### Phase 3.3: Advanced Security (COMPLETE)
- [x] Counting Bloom filters + registry plumbing
- [x] Immutable audit log with BLAKE3 hash chains + TSA hooks
- [x] SPIFFE + mTLS ingress middleware + refreshable allow-list
- [x] Kyber hybrid crypto profile + segment metadata
- [x] Security module + docs aligning Bloom/Audit/PQ/eBPF

### Phase 4: Advanced Protocol Views
- [ ] NVMe-oF block target (SPDK)
- [ ] NFS v4.2 file export
- [ ] FUSE filesystem mount
- [ ] CSI driver for Kubernetes

### Phase 5: Enterprise Features
- [ ] Metro-sync replication
- [ ] Policy compiler
- [ ] Erasure coding (6+2)
- [ ] Hardware offload (DPU/GPU)
- [ ] Confidential compute enclaves

---

## Performance Characteristics

### Compression (Phase 2.1)

| Data Type | Algorithm | Compression Ratio | Throughput |
|-----------|-----------|-------------------|------------|
| Text/logs | Zstd level 3 | 3-5x | ~500 MB/s |
| Binary/mixed | LZ4 level 1 | 1.5-2.5x | ~2 GB/s |
| Random/encrypted | None (skipped) | 1.0x | ~5 GB/s |

### Deduplication (Phase 2.2)

| Scenario | Dedup Ratio | Space Saved |
|----------|-------------|-------------|
| VM images (identical) | 10-20x | 90-95% |
| Log files (repeated) | 2-5x | 50-80% |
| User data (mixed) | 1.5-3x | 30-65% |
| Unique data | 1.0x | 0% |

### Encryption (Phase 3.1)

| Operation | Baseline | With Encryption | Overhead |
|-----------|----------|-----------------|----------|
| Write | 2.1 GB/s | 2.0 GB/s | +5% |
| Read | 3.5 GB/s | 3.2 GB/s | +9% |
| Dedup | Works | **Still Works** | 0% impact |

**Breakdown per 4MB segment:**
```
Compression (LZ4):     ~0.5ms  (2.5 GB/s)
Hashing (BLAKE3):      ~0.3ms  (13 GB/s)
Encryption (XTS-AES):  ~0.8ms  (5 GB/s with AES-NI)
MAC (BLAKE3):          ~0.3ms  (13 GB/s)
NVRAM write:           ~0.1ms  (fsync)
--------------------------------
Total:                 ~2.0ms per 4MB segment
```

### Overhead Summary

- Hash computation (BLAKE3): ~2ms per 4MB segment
- Content store lookup: <1us (HashMap)
- Encryption overhead: <5% of write time
- MAC overhead: <1% of write time
- Dedup overhead: <1% of write time
- **Combined overhead: <10% increase in write latency**

---

## Contributing

This is an experimental platform exploring radical new storage architectures. We welcome:

-  Bug reports and fixes
-  Architecture suggestions
-  Documentation improvements
-  New test cases
-  Performance optimizations
-  Security reviews

**Before submitting PRs:**
1. Run `cargo fmt` and `cargo clippy`
2. Ensure all tests pass (`cargo test --workspace`)
3. Update documentation for new features
4. Add tests for new functionality

---

## Learn More

- **[Architecture Overview](docs/architecture.md)** -- Full system design
- **[Future State Architecture](docs/future_state_architecture.md)** -- Vision and roadmap
- **[Patentable Concepts](docs/patentable_concepts.md)** - Novel mechanisms
- **[Dedup Implementation](docs/DEDUP_IMPLEMENTATION.md)** - Phase 2.2 technical details
- **[Encryption Implementation](docs/ENCRYPTION_IMPLEMENTATION.md)** - **NEW: Phase 3 security details**
- **[Protocol Views Integration](docs/protocol_views.md)** - CLI workflow for S3/NFS/block facades
- **[S3 Quick Start](QUICKSTART_S3.md)** -- Protocol view demo
- **[Build Guide](BUILD.md)** -- Compilation and testing

---

## License

**Apache 2.0** -> Permissive open source license with patent grant

- Open use: commercial, research, and personal deployments are all allowed
- Keep notices: retain copyright and license text when distributing
- Contributions: submitted patches are accepted under Apache 2.0 (see CONTRIBUTING.md)

[Full license details ->](LICENSE) | [Contribution guidelines ->](CONTRIBUTING.md)

### Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work shall be licensed as above, without any additional terms or conditions.

---

## Project Status

**Current Phase:** Phase 3.3 Complete (Advanced Security)  
**Stability:** Experimental -- API subject to change  
**Production Ready:** Not yet (educational/research purposes)  

**What works today:**
- [x] Capsule storage with compression and deduplication
- ? Counting Bloom + audit log (feature `advanced-security`)
- ? SPIFFE + mTLS gateway with optional eBPF + Kyber toggle
- [x] **XTS-AES-256 encryption with integrity verification**
- [x] **Deterministic encryption preserving deduplication**
- [x] **Key management with rotation support**
- [x] S3-compatible REST API
- [x] CLI tools for basic operations
- [x] Persistent metadata and NVRAM log

**Known limitations:**
- Log-space reclamation pending (Phase 4)
- ! CLI doesn't have --encrypt flag yet (Phase 3.2)
- ! Single-node only (clustering = Phase 5)
- ! No authentication/authorization (Phase 4)

---

## Quick Demo
```bash
# Build
cargo build --release

# Setup encryption (optional)
export SPACE_MASTER_KEY=$(openssl rand -hex 32)

# Create a file with repeated content
echo "SPACE STORAGE PLATFORM" > demo.txt
for i in {1..1000}; do echo "SPACE STORAGE PLATFORM" >> demo.txt; done

# First capsule - no dedup yet
./target/release/spacectl create --file demo.txt

# Second capsule - watch the dedup magic!
./target/release/spacectl create --file demo.txt

# Expected output:
# *  Dedup hit: Reusing segment 0 (saved 24576 bytes)
#  Segment 1: encrypted with key v1 (if SPACE_MASTER_KEY set)
# [x] Capsule ...: 5.2x compression, 1 dedup hits (24576 bytes saved)  encrypted

# Start S3 server
./target/release/spacectl serve-s3 --port 8080 &

# Access via S3 API
curl -X PUT http://localhost:8080/demo/test.txt -d "Hello SPACE!"
curl http://localhost:8080/demo/test.txt
```

### Explore NFS and Block views
```powershell
# Create directories and write a file via the NFS view
spacectl nfs mkdir --path /lab/results
spacectl nfs write --path /lab/results/report.json --file report.json
spacectl nfs list --path /lab/results
spacectl nfs read --path /lab/results/report.json > fetched.json

# Provision a 32MiB block volume and write a sector
spacectl block create vol1 33554432
spacectl block write vol1 4096 --file sector.bin
spacectl block read vol1 4096 --length 512 > sector.verify
spacectl block delete vol1
```

### Telemetry & Logging
- `SPACE_LOG_FORMAT` controls console output (`compact` by default, set to `json` for structured logs).
- `RUST_LOG` follows `tracing` filters (example: `RUST_LOG=info,space=debug`).
- All pipeline stages emit structured spans/events (`pipeline::compression`, `telemetry::compression`) for the future telemetry hub.
- Primary error surfaces for on-call runbooks:
  | Code | Level | Description | Suggested Action |
  |------|-------|-------------|------------------|
  | `CompressionError::EntropySkip` | `WARN` | High-entropy payload skipped compression, includes entropy + segment size | Optional; review workload mix if persistent |
  | `CompressionError::IneffectiveRatio` | `INFO` | Compression reverted due to poor ratio, includes achieved ratio | Tune policy thresholds if noisy |
  | `PipelineError::Compression` | `ERROR` | Compression subsystem hard-failed for a segment (includes index) | Retry segment; inspect codec health |
  | `PipelineError::Nvram` / `PipelineError::Registry` | `ERROR` | Storage metadata IO failure with operation identifier | Investigate backing store, retry once safe |
  | `PipelineError::Telemetry` | `WARN` | Downstream telemetry sink rejected structured event | Defer to hub health; logs still written locally |

---

<div align="center">

**Built with  Rust**

*Breaking storage silos, one encrypted capsule at a time.*

**Phase 3.3 Complete: Compression ? | Dedup ? | Protocol Views ? | Advanced Security ?**

[Report Bug](https://github.com/your-org/space/issues) - [Request Feature](https://github.com/your-org/space/issues) - [Discussions](https://github.com/your-org/space/discussions)

</div>




